from __future__ import annotations

import os
import re
import time

import orjson
import requests

from pathlib import Path
from subprocess import Popen

from ..default import get_config, get_homedir
from ..helpers import fromisoformat_wrapper

from .abstract_feeder import AbstractFeeder


class CSAFGeneric(AbstractFeeder):
    def __init__(self, feeder_name: str) -> None:
        super().__init__(feeder_name)

        csaf_downloader = get_config('generic', 'csaf_downloader_path')
        if not csaf_downloader or not os.path.exists(csaf_downloader):
            self.logger.error('A CSAF downloader is required for this feeder')
            raise FileNotFoundError('CSAF downloader not found')

        if not hasattr(self, 'csaf_metadata'):
            self.logger.error('The URL to the metadata file is required for this feeder')
            raise Exception('Link to the metadata CSAF repository is missing.')

        if not hasattr(self, 'file_pattern'):
            self.file_pattern = '.*json$'
        self.init_csaf_repo()

    def init_csaf_repo(self) -> None:
        csaf_downloader = get_config('generic', 'csaf_downloader_path')
        if not csaf_downloader or not os.path.exists(csaf_downloader):
            self.logger.error('A CSAF downloader is required for this feeder')
            raise FileNotFoundError('CSAF downloader not found')

        self.path_to_repo = get_homedir() / 'vulnerabilitylookup' / 'feeders' / self.name
        if not self.path_to_repo.exists():
            self.pull_csaf_repo()

    def pull_csaf_repo(self, last_update: str | None = None) -> None:
        csaf_downloader = get_config('generic', 'csaf_downloader_path')
        if not csaf_downloader or not os.path.exists(csaf_downloader):
            self.logger.error('A CSAF downloader is required for this feeder')
            raise FileNotFoundError('CSAF downloader not found')

        if not hasattr(self, 'csaf_metadata'):
            self.logger.error('The URL to the metadata file is required for this feeder')
            raise Exception('Link to the metadata CSAF repository is missing.')

        to_run = [csaf_downloader, '-d', self.path_to_repo]
        if last_update:
            to_run.append('-t')
            to_run.append(last_update)
        to_run.append(self.csaf_metadata)
        with Popen(to_run) as process:
            self.logger.info('Downloading CSAF data.')
            while process.poll() is None:
                if last_update:
                    self.logger.info(f'Downloading CSAF data since {last_update}')
                else:
                    self.logger.info('Still downloading CSAF data...')
                time.sleep(10)
            self.logger.info('Downloading CSAF data finished')

    def _get_last_update_from_meta(self) -> str:
        if not hasattr(self, 'csaf_metadata'):
            self.logger.error('The URL to the metadata file is required for this feeder')
            raise Exception('Link to the metadata CSAF repository is missing.')

        try:
            metadata_response = requests.get(self.csaf_metadata)
            metadata = metadata_response.json()
            return metadata['last_updated']
        except Exception as e:
            raise Exception(f'Could not get last update from metadata: {e} - "{metadata_response.text}"')

    def update(self) -> bool:
        # TODO: try to only get the updates. Not sure it is possible
        # but this link might help: https://github.com/csaf-poc/csaf_distribution/blob/main/docs/csaf_downloader.md#timerange-option
        self.init_csaf_repo()
        last_update = self._get_last_update_from_meta()

        paths_to_import: set[Path] = set()
        if _last_update := self.storage.hget('last_updates', self.name):
            _last_update_str = _last_update.decode()
            # Get last time the repo was updated
            if _last_update_str == last_update:
                # No changes
                self.logger.info('No updates.')
                return False

            self.pull_csaf_repo(_last_update.decode())

            # TODO: only load updated files

        paths_to_import = set()
        for tlp_color in self.path_to_repo.iterdir():
            if not tlp_color.is_dir():
                # That's the logfile
                continue
            for year in tlp_color.iterdir():
                for csaf_file in year.iterdir():
                    if re.match(self.file_pattern, csaf_file.name):
                        paths_to_import.add(csaf_file)

        if not paths_to_import:
            return False

        p = self.storage.pipeline()
        csafids: dict[str, float] = {}
        for path in paths_to_import:
            # Store all cves individually
            with path.open() as vuln_entry:
                vuln = orjson.loads(vuln_entry.read())
                modified = fromisoformat_wrapper(vuln['document']['tracking']['current_release_date'])
                vuln_id = path.stem.lower()
                csafids[vuln_id] = modified.timestamp()
                if 'vulnerabilities' in vuln and vuln.get('vulnerabilities'):
                    for _v in vuln.get('vulnerabilities'):
                        if cve := _v.get('cve'):
                            cve = cve.lower()
                            p.sadd(f'{vuln_id}:link', cve)
                            p.sadd(f'{cve}:link', vuln_id)
                p.set(vuln_id, orjson.dumps(vuln))
            if len(csafids) > 1000:
                # Avoid a massive execute on first import
                p.zadd(f'index:{self.name}', csafids)  # type: ignore
                p.zadd('index', csafids)  # type: ignore
                p.execute()

                # reset pipeline
                p = self.storage.pipeline()
                csafids = {}

        if csafids:
            # remaining entries
            p.zadd(f'index:{self.name}', csafids)  # type: ignore
            p.zadd('index', csafids)  # type: ignore
            p.execute()
        self.storage.hset('last_updates', mapping={self.name: last_update})
        self.logger.info('Import done.')
        return True
